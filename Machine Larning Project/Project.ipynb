{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from math import *\n",
    "\n",
    "class gaussNB():\n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "        \n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        \n",
    "        \n",
    "        samples_no, no_features = X_train.shape #get the number of rows and columns using the .shape method\n",
    "        \n",
    "        \n",
    "        self.classes_= np.unique(y_train) #get the number of unique classes in our dataset\n",
    "        \n",
    "        no_classes = len(self.classes)\n",
    "        \n",
    "        #Initialising our mean, variance, and priors\n",
    "        \n",
    "        self.mean_ = np.zeros((no_classes, no_features), dtype = float)\n",
    "        \n",
    "        self.var_ = np.zeros((no_classes, no_features), dtype = float)\n",
    "        self.priors_ = np.zeros(no_classes, dtype = float) #for each class we only need one prior, so we'll use a 1-D array\n",
    "        \n",
    "        for single_class in self.classes_: #where single class is an entry in our list of classes\n",
    "            X_class = X_train[single_class == y_train]\n",
    "            \n",
    "            #we now calculate the mean for each class by filling this row and all the columns\n",
    "            self.mean_[single_class, :] = X_class.mean(axis = 0) #axis = 0 shows that we're iterating over the columns\n",
    "            self.var_[single_class, :] = X_class.var(axis = 0)\n",
    "            self.priors_[single_class] = (X_class.shape[0]/ float(samples_no)) #X_class.shape[0] gets the number of samples with single_class as the label\n",
    "            \n",
    "       \n",
    "    #We now implement a helper method that will enable us to predict the output of a single column \n",
    "    def predictSinglePoint(self, x):\n",
    "        #calculate the posterior probability\n",
    "        \n",
    "        posteriors = []      \n",
    "        \n",
    "        for indx, single_class in enumerate(self.classes_): #get the index and class labels using the enumerate function\n",
    "            prior =np.log(self.priors_[indx]) #we use the priors we calculated in our fit method\n",
    "        \n",
    "            #calculate the class conditionals and the prior for each class \n",
    "            class_conditional = np.log(self.pd_function(indx, x))\n",
    "            \n",
    "            total = np.sum(class_conditional) #sum all our class conditionals\n",
    "            posterior = prior + total\n",
    "            \n",
    "            posteriors.append(posterior)\n",
    "            \n",
    "        #choose the class with the highest probability using the inbuilt numpy argmax function\n",
    "        \n",
    "        output = self.classes_[np.argmax(posteriors)]\n",
    "        \n",
    "        return output\n",
    "            \n",
    "    \n",
    "    #a helper function(probability density function) which will help us with our Gaussian function\n",
    "    \n",
    "    def pd_function(self, class_indx, x):\n",
    "        #use the values we calculated in our fit method\n",
    "        mean = self.mean_[class_indx]\n",
    "        \n",
    "        var = self.var_[class_indx]\n",
    "        \n",
    "        #divide the gauss function into numerator and denominator to avoid confusion\n",
    "        \n",
    "        num = np.exp(-(math.pow(x-mean,2))/(2*(math.pow(var,2))))\n",
    "        denum = math.sqrt(2*math.pi*math.pow(var,2))\n",
    "        \n",
    "        return num/denum\n",
    "     \n",
    "    #this is our main prediction method for predicting multiple samples    \n",
    "    def predict(self, X_test):\n",
    "        result = []\n",
    "        \n",
    "        for x in X_test:\n",
    "            prediction = self.predictSinglePoint(x) #here we get the prediction for each given row\n",
    "            result.append(prediction)               #append the prediction to our list of results\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def acc_score(self, X_test, y_test):\n",
    "        \n",
    "        y_predicted = self.predict(X_test)\n",
    "        score = float(np.sum(y_predicted==y_test)/len(y_test))\n",
    "        \n",
    "        return score\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class multiNB():\n",
    "    \n",
    "    def __init__(self, alpha=1):\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def fit(self, X_train, y_train):\n",
    "        \n",
    "        samples_no, no_features = X_train.shape #get the number of rows and columns using the .shape method\n",
    "        \n",
    "        \n",
    "        self.classes_= np.unique(y_train) #get the number of unique classes in our dataset\n",
    "        \n",
    "        no_classes = len(self.classes)\n",
    "        \n",
    "        #inititalising our priors and likelihoods\n",
    "        \n",
    "        self.priors_ = np.zeros(no_classes)\n",
    "        self.likelihoods_ = np.zeros((no_classes, no_features))\n",
    "        \n",
    "        #here we're finding our priors and likelihoods\n",
    "        \n",
    "        for indx, single_class in enumerate(self.classes_):\n",
    "            X_train_clss = X_train[single_class == y_train]\n",
    "            self.priors_[single_class] = (X_class.shape[0]/ float(samples_no)) #X_class.shape[0] gets the number of samples with single_class as the label\n",
    "            self.likelihoods_[indx, :] = ((X_train_clss.sum(axis=0)) + self.alpha) / (np.sum(X_train_clss.sum(axis=0) + self.alpha)) #we include alpha here to smooth our table of likelihoods\n",
    "            \n",
    "      \n",
    "    def predictSinglePoint(self, x):\n",
    "        #calculate the posterior probability\n",
    "        \n",
    "        posteriors = []      \n",
    "        \n",
    "        for indx, single_class in enumerate(self.classes_): #get the index and class labels using the enumerate function\n",
    "            prior =np.log(self.priors_[indx]) #we use the priors we calculated in our fit method\n",
    "        \n",
    "            #calculate the likelihood and the prior for each class \n",
    "            likelihood_class = np.log(self.likelihoods_[indx, :]) * x\n",
    "            \n",
    "            total = np.sum(likelihood_class) #sum all our class likelihoods\n",
    "            posterior = prior + total\n",
    "            \n",
    "            posteriors.append(posterior)\n",
    "            \n",
    "        #choose the class with the highest probability using the inbuilt numpy argmax function\n",
    "        \n",
    "        output = self.classes_[np.argmax(posteriors)]\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        result = []\n",
    "        \n",
    "        for x in X_test:\n",
    "            prediction = self.predictSinglePoint(x) #here we get the prediction for each given row\n",
    "            result.append(prediction)               #append the prediction to our list of results\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def acc_score(self, X_test, y_test):\n",
    "        \n",
    "        y_predicted = self.predict(X_test)\n",
    "        score = float(np.sum(y_predicted==y_test)/len(y_test))\n",
    "        \n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
